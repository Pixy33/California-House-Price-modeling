{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b8ed7d",
   "metadata": {},
   "source": [
    "This dataset is acquired from the following Kaggle data source:\n",
    "https://www.kaggle.com/datasets/camnugent/california-housing-prices\n",
    "\n",
    "This script has benefited immensely from the following:\n",
    "\n",
    "Julien Solal's excellent kernel on feature engineering\n",
    "https://www.kaggle.com/code/juliencs/a-study-on-regression-applied-to-the-ames-dataset/notebook\n",
    "\n",
    "Serigne Cisse's kernel on the Ames House Price dataset\n",
    "https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard/notebook\n",
    "\n",
    "Cameron Nugent's kernel on Geospatial Feature Engineering\n",
    "https://www.kaggle.com/code/camnugent/geospatial-feature-engineering-and-visualization/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c412553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy.stats import boxcox\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from geopy import distance as dist  \n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "#Force pd to display print in full\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "data = pd.read_csv('housing.csv')\n",
    "#Random reshuffle of index to avoid possible implicit sorting\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "y = data['median_house_value']\n",
    "X = data.drop('median_house_value',axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f2d20",
   "metadata": {},
   "source": [
    "## Data transformation, feature crossing and engineering\n",
    "\n",
    "We begin by exploring our data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe397445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in training set summary: \n",
      " Series([], dtype: float64)\n",
      "Missing data in testing set summary: \n",
      " Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "def cal_isna(df):\n",
    "    '''\n",
    "    Calculate and return a series of missing data ratio, only features with missing data are returned\n",
    "    '''\n",
    "    df_isna_non_zero=(df.isna().sum()/df.shape[0])!=0\n",
    "    df_isna=(df.isna().sum()/df.shape[0])[df_isna_non_zero].sort_values(ascending=False)\n",
    "    return df_isna\n",
    "\n",
    "print('Missing data in training set summary:','\\n',cal_isna(X_train))\n",
    "print('Missing data in testing set summary:','\\n',cal_isna(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae80485",
   "metadata": {},
   "source": [
    "There are only about 1% of data missing, and it is unclear whether they are missing at random, or if there is an underlying reason - very little background info is available for this dataset.\n",
    "Therefore perhaps it is the easiest to simply remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b9830e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_index_train=X_train[X_train['total_bedrooms'].isna()].index\n",
    "missing_index_test=X_test[X_test['total_bedrooms'].isna()].index\n",
    "\n",
    "X_train=X_train.dropna(subset=['total_bedrooms'])\n",
    "y_train=y_train.drop(index=missing_index_train)\n",
    "X_test=X_test.dropna(subset=['total_bedrooms'])\n",
    "y_test=y_test.drop(index=missing_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cfb3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_plot=sns.displot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902618dc",
   "metadata": {},
   "source": [
    "It is very strange that there are over 700 districts with the same median house value (MHV), 500000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4be3820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of MHV values \n",
      " 500001.0    0.047206\n",
      "137500.0    0.005665\n",
      "162500.0    0.005315\n",
      "112500.0    0.005035\n",
      "225000.0    0.004546\n",
      "187500.0    0.004196\n",
      "350000.0    0.003916\n",
      "87500.0     0.003846\n",
      "275000.0    0.003147\n",
      "175000.0    0.002937\n",
      "Name: median_house_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of MHV values','\\n',(y_train.value_counts()/y_train.shape[0]).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da5182",
   "metadata": {},
   "source": [
    "0.048 of the MHV data are 500k, the next most numerous MHV is 137500, with 0.006 of the data, this is definitely not right.\n",
    "\n",
    "I suspect these are MHV that are >500k but are censored - any MHV>500k are categorized as 500k.\n",
    "\n",
    "As it is, I think the best approach to leave them be, as deleting them would bias the model.\n",
    "\n",
    "Furthermore, as random forest regression will be used later, deleting the right censored data would make prediction on any MHV>500k an extrapolation, which RF cannot do.\n",
    "\n",
    "It also seems that the target variable is skewed, let's Box-Cox transform it to tackle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25fbffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda for train= 0.12203858784080607\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train[:], lamb_train = boxcox(y_train)\n",
    "print('Lambda for train=',lamb_train)\n",
    "y_boxcox_plot=sns.displot(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c624b58",
   "metadata": {},
   "source": [
    "The target variable now looks a lot more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5f5e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14299.000000\n",
      "mean      -119.557403\n",
      "std          2.000714\n",
      "min       -124.300000\n",
      "25%       -121.770000\n",
      "50%       -118.480000\n",
      "75%       -118.010000\n",
      "max       -114.470000\n",
      "Name: longitude, dtype: float64\n",
      "count    14299.000000\n",
      "mean        35.624211\n",
      "std          2.139275\n",
      "min         32.540000\n",
      "25%         33.930000\n",
      "50%         34.250000\n",
      "75%         37.720000\n",
      "max         41.950000\n",
      "Name: latitude, dtype: float64\n",
      "count    14299.000000\n",
      "mean        28.641513\n",
      "std         12.523053\n",
      "min          1.000000\n",
      "25%         18.000000\n",
      "50%         29.000000\n",
      "75%         37.000000\n",
      "max         52.000000\n",
      "Name: housing_median_age, dtype: float64\n",
      "count    14299.000000\n",
      "mean      2648.664592\n",
      "std       2197.761720\n",
      "min          6.000000\n",
      "25%       1448.000000\n",
      "50%       2127.000000\n",
      "75%       3159.500000\n",
      "max      32627.000000\n",
      "Name: total_rooms, dtype: float64\n",
      "count    14299.000000\n",
      "mean       541.079446\n",
      "std        426.011754\n",
      "min          1.000000\n",
      "25%        295.000000\n",
      "50%        437.000000\n",
      "75%        652.000000\n",
      "max       6445.000000\n",
      "Name: total_bedrooms, dtype: float64\n",
      "count    14299.000000\n",
      "mean      1433.548150\n",
      "std       1158.745486\n",
      "min          3.000000\n",
      "25%        782.000000\n",
      "50%       1171.000000\n",
      "75%       1728.000000\n",
      "max      35682.000000\n",
      "Name: population, dtype: float64\n",
      "count    14299.000000\n",
      "mean       502.683055\n",
      "std        387.867405\n",
      "min          1.000000\n",
      "25%        279.000000\n",
      "50%        411.000000\n",
      "75%        609.000000\n",
      "max       6082.000000\n",
      "Name: households, dtype: float64\n",
      "count    14299.000000\n",
      "mean         3.870628\n",
      "std          1.900676\n",
      "min          0.499900\n",
      "25%          2.562500\n",
      "50%          3.539100\n",
      "75%          4.736100\n",
      "max         15.000100\n",
      "Name: median_income, dtype: float64\n",
      "count         14299\n",
      "unique            5\n",
      "top       <1H OCEAN\n",
      "freq           6347\n",
      "Name: ocean_proximity, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for feature in X_train.columns:\n",
    "    print(X_train[feature].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd893104",
   "metadata": {},
   "source": [
    "It maybe a good idea to cross both longitude and latitude to create a new feature.\n",
    "\n",
    "Crossing a numeric feature would result in an explosion of feature space.\n",
    "So let's bucket them before creating a boolean feature which indicates a region's longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ff33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_bins=[long for long in range(-125,-110,5)]\n",
    "X_train[\"longitude_buc\"] = pd.cut(X_train.longitude, long_bins,include_lowest=True)\n",
    "lat_bins=[lat for lat in range(30,45,5)]\n",
    "X_train[\"latitude_buc\"] = pd.cut(X_train.latitude, lat_bins,include_lowest=True)\n",
    "\n",
    "X_train['lat:(29.999, 35.0] & long:(-125.001, -120.0]'] = ((X_train[\"latitude_buc\"]==pd.Interval(29.999, 35.0, closed='right'))&(X_train[\"longitude_buc\"]==pd.Interval(-125.001, -120.0, closed='right'))).astype(int)\n",
    "X_train['lat:(29.999, 35.0] & long:(-120.0, -115.0]'] = ((X_train[\"latitude_buc\"]==pd.Interval(29.999, 35.0, closed='right'))&(X_train[\"longitude_buc\"]==pd.Interval(-120.0, -115.0, closed='right'))).astype(int)\n",
    "X_train['lat:(35.0, 40.0] & long:(-125.001, -120.0]'] = ((X_train[\"latitude_buc\"]==pd.Interval(35.0, 40.0, closed='right'))&(X_train[\"longitude_buc\"]==pd.Interval(-125.001, -120.0, closed='right'))).astype(int)\n",
    "X_train['lat:(35.0, 40.0] & long:(-120.0, -115.0]'] = ((X_train[\"latitude_buc\"]==pd.Interval(35.0, 40.0, closed='right'))&(X_train[\"longitude_buc\"]==pd.Interval(-120.0, -115.0, closed='right'))).astype(int)\n",
    "\n",
    "X_test[\"longitude_buc\"] = pd.cut(X_test.longitude, long_bins,include_lowest=True)\n",
    "X_test[\"latitude_buc\"] = pd.cut(X_test.latitude, lat_bins,include_lowest=True)\n",
    "X_test['lat:(29.999, 35.0] & long:(-125.001, -120.0]'] = ((X_test[\"latitude_buc\"]==pd.Interval(29.999, 35.0, closed='right'))&(X_test[\"longitude_buc\"]==pd.Interval(-125.001, -120.0, closed='right'))).astype(int)\n",
    "X_test['lat:(29.999, 35.0] & long:(-120.0, -115.0]'] = ((X_test[\"latitude_buc\"]==pd.Interval(29.999, 35.0, closed='right'))&(X_test[\"longitude_buc\"]==pd.Interval(-120.0, -115.0, closed='right'))).astype(int)\n",
    "X_test['lat:(35.0, 40.0] & long:(-125.001, -120.0]'] = ((X_test[\"latitude_buc\"]==pd.Interval(35.0, 40.0, closed='right'))&(X_test[\"longitude_buc\"]==pd.Interval(-125.001, -120.0, closed='right'))).astype(int)\n",
    "X_test['lat:(35.0, 40.0] & long:(-120.0, -115.0]'] = ((X_test[\"latitude_buc\"]==pd.Interval(35.0, 40.0, closed='right'))&(X_test[\"longitude_buc\"]==pd.Interval(-120.0, -115.0, closed='right'))).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c01ee",
   "metadata": {},
   "source": [
    "We may also need to encode ocean_proximity because it is a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f4bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<1H OCEAN     6347\n",
      "INLAND        4547\n",
      "NEAR OCEAN    1838\n",
      "NEAR BAY      1563\n",
      "ISLAND           4\n",
      "Name: ocean_proximity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train['ocean_proximity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac309252",
   "metadata": {},
   "source": [
    "There is an order of preference built into these categories, with ISLAND being the most desirable and <1H OCEAN being the least desirable, let's ordinal encode this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d19f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.replace({\"ocean_proximity\" : {\"ISLAND\" : 5, \"NEAR BAY\" : 4, \n",
    "                                                \"NEAR OCEAN\" : 3,\n",
    "                                                \"INLAND\" : 2,\n",
    "                                                \"<1H OCEAN\" : 1}})\n",
    "X_test = X_test.replace({\"ocean_proximity\" : {\"ISLAND\" : 5, \"NEAR BAY\" : 4, \n",
    "                                                \"NEAR OCEAN\" : 3,\n",
    "                                                \"INLAND\" : 2,\n",
    "                                                \"<1H OCEAN\" : 1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d757e3",
   "metadata": {},
   "source": [
    "Let's also cross the amount of rooms and bedrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81ccc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['rooms x bedrooms']=X_train['total_rooms']*X_train['total_bedrooms']\n",
    "X_test['rooms x bedrooms']=X_test['total_rooms']*X_test['total_bedrooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5bd54",
   "metadata": {},
   "source": [
    "The numeric features are also on different scales, so we should scale them too.\n",
    "But we should leave it till afterwards because we will create polynomial features. One of which would be sqrt.\n",
    "\n",
    "Let's see how correlated the features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bafdba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_house_value                              1.000000\n",
      "median_income                                   0.661811\n",
      "lat:(29.999, 35.0] & long:(-120.0, -115.0]      0.201173\n",
      "total_rooms                                     0.149674\n",
      "households                                      0.095614\n",
      "housing_median_age                              0.083938\n",
      "total_bedrooms                                  0.076574\n",
      "rooms x bedrooms                                0.059297\n",
      "lat:(35.0, 40.0] & long:(-125.001, -120.0]      0.029677\n",
      "population                                      0.012081\n",
      "ocean_proximity                                -0.012723\n",
      "lat:(29.999, 35.0] & long:(-125.001, -120.0]   -0.014737\n",
      "longitude                                      -0.018865\n",
      "latitude                                       -0.194132\n",
      "lat:(35.0, 40.0] & long:(-120.0, -115.0]       -0.351873\n",
      "Name: median_house_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_corr_df = X_train.copy()\n",
    "train_corr_df['median_house_value']=y_train\n",
    "corr_table=train_corr_df.corr()['median_house_value'].sort_values(ascending=False)\n",
    "print(corr_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bef18a",
   "metadata": {},
   "source": [
    "Let's create new features by making polynomial features out of the top 5 most correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "734a5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features=['median_income','total_rooms','households','total_bedrooms','housing_median_age']\n",
    "\n",
    "def polynomial_features(df,features):\n",
    "    '''\n",
    "    Takes a df and a list of features and returns a df with polynomial features\n",
    "    created out of those features\n",
    "    '''\n",
    "    for feature in features:\n",
    "        name_sq=feature+str('**2')\n",
    "        df[name_sq]=df[feature]**2\n",
    "        \n",
    "        name_cube=feature+str('**3')\n",
    "        df[name_cube]=df[feature]**3\n",
    "        \n",
    "        name_sqrt=feature+str(' sqrt')\n",
    "        df[name_sqrt]=np.sqrt(df[feature])\n",
    "    return df\n",
    "\n",
    "X_train=polynomial_features(X_train,poly_features)\n",
    "X_test=polynomial_features(X_test,poly_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3093900",
   "metadata": {},
   "source": [
    "We should be aware of skewness, because later we will be using Random Forest regressor, which is rather sensitive to skewness.\n",
    "Again let's perform a Boxcox transformation on the skewed numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d862e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 skewed numerical features to transform\n",
      "They are ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
      "5 skewed numerical features to transform\n",
      "They are ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n"
     ]
    }
   ],
   "source": [
    "def check_skew_boxcox(df,features,threshold=0.5):\n",
    "    '''\n",
    "    Check for skewed feature and return df with skewed features transformed\n",
    "\n",
    "    '''\n",
    "    from scipy.stats import skew\n",
    "    skewness = df[features].apply(lambda x: skew(x))\n",
    "    skewness = skewness[abs(skewness) > 0.5]\n",
    "    print(str(skewness.shape[0]) + \" skewed numerical features to transform\")\n",
    "    print('They are',[feature for feature in skewness.index])\n",
    "    for feature in skewness.index:\n",
    "        df[feature]=boxcox(df[feature])[0]\n",
    "    return df\n",
    "\n",
    "features=['housing_median_age', 'total_rooms',\n",
    "        'total_bedrooms', 'population', 'households', 'median_income']\n",
    "X_train=check_skew_boxcox(X_train,features)\n",
    "X_test=check_skew_boxcox(X_test,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657c621",
   "metadata": {},
   "source": [
    "Now we scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52bbe40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=['longitude', 'latitude','housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income']\n",
    "cat_features=['ocean_proximity', 'longitude_buc', 'latitude_buc',\n",
    "'lat:(29.999, 35.0] & long:(-125.001, -120.0]',\n",
    "'lat:(29.999, 35.0] & long:(-120.0, -115.0]',\n",
    "'lat:(35.0, 40.0] & long:(-125.001, -120.0]',\n",
    "'lat:(35.0, 40.0] & long:(-120.0, -115.0]']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#Scaling all numeric features except longitude and latitude, as that may distort\n",
    "#the location\n",
    "X_train_for_scaling=X_train[num_features].drop(['longitude', 'latitude'],axis=1)\n",
    "X_train_scaling_cols=X_train_for_scaling.columns\n",
    "X_train_scaling_index=X_train_for_scaling.index\n",
    "X_train_for_scaling=scaler.fit_transform(X_train_for_scaling)\n",
    "X_train_for_scaling = pd.DataFrame(X_train_for_scaling,columns=X_train_scaling_cols,index=X_train_scaling_index)\n",
    "for feature in X_train_for_scaling.columns:\n",
    "    X_train[feature]=X_train_for_scaling[feature].values\n",
    "\n",
    "X_test_for_scaling=X_test[num_features].drop(['longitude', 'latitude'],axis=1)\n",
    "X_test_scaling_cols=X_test_for_scaling.columns\n",
    "X_test_scaling_index=X_test_for_scaling.index\n",
    "X_test_for_scaling=scaler.transform(X_test_for_scaling)\n",
    "X_test_for_scaling=pd.DataFrame(X_test_for_scaling,columns=X_test_scaling_cols,index=X_test_scaling_index)\n",
    "for feature in X_test_for_scaling.columns:\n",
    "    X_test[feature]=X_test_for_scaling[feature].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf781c7",
   "metadata": {},
   "source": [
    "## Geospatial feature engineering\n",
    "\n",
    "We now move onto engineering geospatial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6e5a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lat_long = pd.read_csv('cal_cities_lat_long.csv')\n",
    "city_pop_data = pd.read_csv('cal_populations_city.csv')\n",
    "county_pop_data = pd.read_csv('cal_populations_county.csv')\n",
    "    \n",
    "\n",
    "city_coords = {}\n",
    "for dat in city_lat_long.iterrows():\n",
    "    row = dat[1]\n",
    "    if row['Name'] not in city_pop_data['City'].values:   \n",
    "        continue           \n",
    "    else: \n",
    "        city_coords[row['Name']] = (float(row['Latitude']), float(row['Longitude']))\n",
    "\n",
    "def closest_point(location, location_dict):\n",
    "    \"\"\" take a tuple of latitude and longitude and \n",
    "        compare to a dictonary of locations where\n",
    "        key = location name and value = (lat, long)\n",
    "        returns tuple of (closest_location , distance) \"\"\"\n",
    "    closest_location = None\n",
    "    for city in location_dict.keys():\n",
    "        distance = dist.geodesic(location, location_dict[city]).kilometers\n",
    "        if closest_location is None:\n",
    "            closest_location = (city, distance)\n",
    "        elif distance < closest_location[1]:\n",
    "            closest_location = (city, distance)\n",
    "    return closest_location\n",
    "\n",
    "\n",
    "city_pop_dict = {}\n",
    "for dat in city_pop_data.iterrows():\n",
    "    row = dat[1]\n",
    "    city_pop_dict[row['City']] =  row['pop_april_1990']\n",
    "\n",
    "\n",
    "big_cities = {}\n",
    "for key, value in city_coords.items():\n",
    "    if city_pop_dict[key] > 500000:\n",
    "        big_cities[key] = value\n",
    "        \n",
    "def add_closest_city(df,city_coords,big_cities):\n",
    "    df['close_city'] = df.apply(lambda x: \n",
    "    \t\t\t\t\t\t\t closest_point((x['latitude'],x['longitude']),city_coords), axis = 1)\n",
    "    df['close_city_name'] = [x[0] for x in df['close_city'].values]\n",
    "    df['close_city_dist'] = [x[1] for x in df['close_city'].values]\n",
    "    df['close_city_pop'] = [city_pop_dict[x] for x in df['close_city_name'].values]\n",
    "    \n",
    "    df = df.drop(['close_city','close_city_name'], axis=1)\n",
    "    \n",
    "    #add the data relating to the points to the closest big city\n",
    "    df['big_city'] = df.apply(lambda x: \n",
    "    \t\t\t\t\t\t\tclosest_point((x['latitude'],x['longitude']),big_cities), axis = 1)\n",
    "    df['big_city_name'] = [x[0] for x in df['big_city'].values]\n",
    "    df['big_city_dist'] = [x[1] for x in df['big_city'].values]\n",
    "\n",
    "    df = df.drop('big_city', axis=1)\n",
    "    return df\n",
    "\n",
    "X_train=add_closest_city(X_train,city_coords,big_cities)\n",
    "X_test=add_closest_city(X_test,city_coords,big_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8aaf9",
   "metadata": {},
   "source": [
    "The feature 'close_city_name' has >400 cities, since it is categorical we must one hot encode it; but if we do so we will have >400 features.\n",
    "Therefore I have decided to exclude this feature.\n",
    "Perhaps in a latter version we can include this feature and use dimension reduction such as PCA to reduce the feature space.\n",
    "\n",
    "Since 'big_city_name' is categorical, we should one hot encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f83a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=OneHotEncoder(sparse=False)\n",
    "ohe_data=X_train[['big_city_name']]\n",
    "X_train_ohe=(ohe.fit_transform(ohe_data)).astype(int)\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe,columns=ohe.get_feature_names(ohe_data.columns),index=ohe_data.index)\n",
    "X_train=pd.concat([X_train,X_train_ohe], axis=1)\n",
    "\n",
    "ohe=OneHotEncoder(sparse=False)\n",
    "ohe_data=X_test[['big_city_name']]\n",
    "X_test_ohe=(ohe.fit_transform(ohe_data)).astype(int)\n",
    "X_test_ohe = pd.DataFrame(X_test_ohe,columns=ohe.get_feature_names(ohe_data.columns),index=ohe_data.index)\n",
    "X_test=pd.concat([X_test,X_test_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d3ac1",
   "metadata": {},
   "source": [
    "The buckets only serve as intermediaries for feature crossing, so they are no longer useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d90127c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.drop(['big_city_name','longitude_buc','latitude_buc'],axis=1)\n",
    "X_test=X_test.drop(['big_city_name','longitude_buc','latitude_buc'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da0b39",
   "metadata": {},
   "source": [
    "## Data modeling\n",
    "\n",
    "Before we model the data, let's define the RMSE scorer.\n",
    "Recall that our target feature has been boxcox transformed, therefore to give a RMSE that has the same scale as the original data, we must revert the transformation.\n",
    "\n",
    "We should also perform the same right censoring as the original data did so that any prediction >500k are not penalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "700959ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def censor_inv_boxcox_rmse(X,y,model,lamb):\n",
    "    pred=inv_boxcox(model.predict(X),lamb)\n",
    "    pred=np.where(pred>500000,500000,pred)\n",
    "\n",
    "    mse = np.mean(((pred - y)**2))\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee6960",
   "metadata": {},
   "source": [
    "Removing invalid charecters from feature names so that XGBoost can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6b887c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "X_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6baed2",
   "metadata": {},
   "source": [
    "Now we begin the modelling. There will be 3 models tried: XBGRegressor, Random Forest Regressor, and a Voting Regression that ensembles the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1986cb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:08:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:08:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:09:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"scale_pos_weight\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:09:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"colsample_bylevel\", \"colsample_bynode\", \"colsample_bytree\", \"gamma\", \"interaction_constraints\", \"max_delta_step\", \"max_depth\", \"min_child_weight\", \"monotone_constraints\", \"num_parallel_tree\", \"predictor\", \"subsample\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Best params for XGB= {'subsample': 1.0, 'objective': 'reg:squarederror', 'n_estimators': 500, 'min_child_weight': 4, 'max_depth': 4, 'gamma': 0.5, 'eval_metric': 'rmse', 'eta': 0.5, 'colsample_bytree': 0.7, 'booster': 'gbtree'}\n",
      "RMSE(xgb_tuned) on Training set : 229426.68614694828\n",
      "RMSE(xgb_tuned) on Test set : 49538.87053235962\n",
      "Best params for RF= {'n_estimators': 20, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'auto', 'max_depth': 90, 'bootstrap': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE(rf_tuned) on Training set : 228623.35830082992\n",
      "RMSE(rf_tuned) on Test set : 49751.89107244583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:285: UserWarning: The total space of parameters 3 is smaller than n_iter=10. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for vr= {'weights': (1, 1)}\n",
      "RMSE(vr_tuned) on Training set : 230142.28711968698\n",
      "RMSE(vr_tuned) on Test set : 46371.4206778783\n"
     ]
    }
   ],
   "source": [
    "xgb=XGBRegressor(tree_method='gpu_hist', nthread=multiprocessing.cpu_count())\n",
    "xgb.fit(X_train,y_train)\n",
    "xgb_params={'n_estimators':[500],\n",
    "    'min_child_weight':[4,5], \n",
    "    'gamma':[i/10.0 for i in range(3,6)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror', 'reg:tweedie'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['rmse'],\n",
    "    'eta': [i/10.0 for i in range(3,6)]}\n",
    "rdm_xgb=RandomizedSearchCV(xgb,\n",
    "                           param_distributions=xgb_params,\n",
    "                           cv=5, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           error_score='raise')\n",
    "rdm_xgb.fit(X_train,y_train)\n",
    "print('Best params for XGB=',rdm_xgb.best_params_)\n",
    "params=rdm_xgb.best_params_\n",
    "xgb_tuned=XGBRegressor(tree_method='gpu_hist', \n",
    "                       nthread=multiprocessing.cpu_count(),\n",
    "                       n_estimators=params['n_estimators'], \n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       gamma=params['gamma'],\n",
    "                       subsample=params['subsample'],\n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       max_depth=params['max_depth'],\n",
    "                       objective=params['objective'],\n",
    "                       booster=params['booster'],\n",
    "                       eval_metric=params['eval_metric'],\n",
    "                       eta=params['eta'])\n",
    "xgb_tuned.fit(X_train,y_train)\n",
    "\n",
    "print(\"RMSE(xgb_tuned) on Training set :\", censor_inv_boxcox_rmse(X_train,y_train,xgb_tuned,lamb_train))\n",
    "print(\"RMSE(xgb_tuned) on Test set :\", censor_inv_boxcox_rmse(X_test,y_test,xgb_tuned,lamb_train))\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "rf_params={'n_estimators': [5,20,50,100],\n",
    "           'max_features': ['auto', 'sqrt'],\n",
    "           'max_depth': [int(x) for x in np.linspace(10, 120, num = 12)],\n",
    "           'min_samples_split': [2, 6, 10],\n",
    "           'min_samples_leaf': [1, 3, 4],\n",
    "           'bootstrap': [True, False]}\n",
    "rdm_rf=RandomizedSearchCV(rf,\n",
    "                           param_distributions=rf_params,\n",
    "                           cv=5, \n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           error_score='raise')\n",
    "rdm_rf.fit(X_train,y_train)\n",
    "print('Best params for RF=',rdm_rf.best_params_)\n",
    "params=rdm_rf.best_params_\n",
    "rf_tuned=RandomForestRegressor(n_estimators=params['n_estimators'], \n",
    "                       max_features=params['max_features'],\n",
    "                       max_depth=params['max_depth'],\n",
    "                       min_samples_split=params['min_samples_split'],\n",
    "                       min_samples_leaf=params['min_samples_leaf'],\n",
    "                       bootstrap=params['bootstrap'])\n",
    "rf_tuned.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(\"RMSE(rf_tuned) on Training set :\", censor_inv_boxcox_rmse(X_train,y_train,rf_tuned,lamb_train))\n",
    "print(\"RMSE(rf_tuned) on Test set :\", censor_inv_boxcox_rmse(X_test,y_test,rf_tuned,lamb_train))\n",
    "\n",
    "\n",
    "\n",
    "vr=VotingRegressor([('xgb_tuned',xgb),('rf_tuned',rf)])\n",
    "vr.fit(X_train,y_train)\n",
    "vr_params={'weights':[(1,1),(2,1),(1,2)]}\n",
    "rdm_vr=RandomizedSearchCV(vr,vr_params)\n",
    "rdm_vr.fit(X_train,y_train)\n",
    "print('Best params for vr=',rdm_vr.best_params_)\n",
    "params=rdm_vr.best_params_\n",
    "vr_tuned=VotingRegressor([('xgb_tuned',xgb),('rf_tuned',rf)],weights=params['weights'])\n",
    "vr_tuned.fit(X_train,y_train)\n",
    "\n",
    "print(\"RMSE(vr_tuned) on Training set :\", censor_inv_boxcox_rmse(X_train,y_train,vr_tuned,lamb_train))\n",
    "print(\"RMSE(vr_tuned) on Test set :\", censor_inv_boxcox_rmse(X_test,y_test,vr_tuned,lamb_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf050bf7",
   "metadata": {},
   "source": [
    "The RMSE on the training set in general is much lower than the test set, suggesting the model may be overfitting. I've experimented with SelectKBest to reduce the amount of features but I cannot improve the overfitting without underming the test set RMSE significantly.\n",
    "\n",
    "The lowest RMSE obtained on this dataset is around 40000 (by Nugent also in the same geospatial feature engineering kernel as mentioned in the beginning), however he seems to have included and one-hot-encoded the feature 'close_city_name', which would have resulted in a feature space of >400. \n",
    "\n",
    "In light of that, I think this test set RMSE is decent, although there is probably room for improvement regarding the overfitting. This will be the focus of the future version of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
